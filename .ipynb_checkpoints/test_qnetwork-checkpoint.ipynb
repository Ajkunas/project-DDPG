{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79fc81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "from qnetwork2 import ReplayBuffer, QNetwork\n",
    "from heuristic import HeuristicPendulumAgent\n",
    "from matplotlib import pyplot\n",
    "import torch.optim as optim\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0dec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "norm_env = NormalizedEnv(env) # accept actions between -1 and 1\n",
    "\n",
    "#we fix a torque\n",
    "torque = norm_env.action(norm_env.action_space.sample())\n",
    "agent = HeuristicPendulumAgent(norm_env, torque)\n",
    "\n",
    "buffer = ReplayBuffer(10000)\n",
    "batch_size = 128\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "hidden_size = 256 # choose as you wish \n",
    "critic = QNetwork(num_states + num_actions, hidden_size, num_actions, agent)\n",
    "optimizer = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "\n",
    "losses = []\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fbc883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, loss: 19.427560806274414, reward: -2.49 \n",
      "episode: 1, loss: 19.610759735107422, reward: -1.97 \n",
      "episode: 2, loss: 19.56987953186035, reward: -2.54 \n",
      "episode: 3, loss: 19.635753631591797, reward: -2.48 \n",
      "episode: 4, loss: 19.62099266052246, reward: -2.47 \n",
      "episode: 5, loss: 19.510387420654297, reward: -2.46 \n",
      "episode: 6, loss: 19.7239933013916, reward: -2.58 \n",
      "episode: 7, loss: 19.561967849731445, reward: -1.98 \n",
      "episode: 8, loss: 19.734922409057617, reward: -3.07 \n",
      "episode: 9, loss: 19.247848510742188, reward: -1.31 \n",
      "episode: 10, loss: 18.823850631713867, reward: -1.96 \n",
      "episode: 11, loss: 19.365005493164062, reward: -1.96 \n",
      "episode: 12, loss: 19.069969177246094, reward: -1.97 \n",
      "episode: 13, loss: 19.171710968017578, reward: -1.89 \n",
      "episode: 14, loss: 18.99932098388672, reward: -2.53 \n",
      "episode: 15, loss: 18.83452033996582, reward: -2.46 \n",
      "episode: 16, loss: 18.73809242248535, reward: -1.97 \n",
      "episode: 17, loss: 18.939746856689453, reward: -2.5 \n",
      "episode: 18, loss: 18.93325424194336, reward: -1.67 \n",
      "episode: 19, loss: 18.859621047973633, reward: -2.59 \n",
      "episode: 20, loss: 19.2342586517334, reward: -3.1 \n",
      "episode: 21, loss: 19.373462677001953, reward: -2.56 \n",
      "episode: 22, loss: 18.90233612060547, reward: -2.03 \n",
      "episode: 23, loss: 19.037109375, reward: -1.9 \n",
      "episode: 24, loss: 19.2219295501709, reward: -1.3 \n",
      "episode: 25, loss: 18.79433822631836, reward: -2.44 \n",
      "episode: 26, loss: 19.163307189941406, reward: -2.5 \n",
      "episode: 27, loss: 19.5750789642334, reward: -2.95 \n",
      "episode: 28, loss: 19.16706085205078, reward: -1.38 \n",
      "episode: 29, loss: 18.3538818359375, reward: -3.1 \n",
      "episode: 30, loss: 18.792362213134766, reward: -1.97 \n",
      "episode: 31, loss: 18.654470443725586, reward: -2.33 \n",
      "episode: 32, loss: 18.69194793701172, reward: -3.13 \n",
      "episode: 33, loss: 18.653989791870117, reward: -1.31 \n",
      "episode: 34, loss: 18.74764633178711, reward: -1.95 \n",
      "episode: 35, loss: 18.714696884155273, reward: -1.95 \n",
      "episode: 36, loss: 19.17285919189453, reward: -2.98 \n",
      "episode: 37, loss: 18.74175262451172, reward: -1.3 \n",
      "episode: 38, loss: 18.506725311279297, reward: -1.95 \n",
      "episode: 39, loss: 19.1791934967041, reward: -2.49 \n",
      "episode: 40, loss: 18.563365936279297, reward: -2.58 \n",
      "episode: 41, loss: 19.22341537475586, reward: -1.94 \n",
      "episode: 42, loss: 18.796354293823242, reward: -2.35 \n",
      "episode: 43, loss: 19.290027618408203, reward: -1.95 \n",
      "episode: 44, loss: 18.9954891204834, reward: -1.95 \n",
      "episode: 45, loss: 19.148286819458008, reward: -2.45 \n",
      "episode: 46, loss: 20.00739097595215, reward: -3.11 \n",
      "episode: 47, loss: 19.833311080932617, reward: -2.48 \n",
      "episode: 48, loss: 19.50450897216797, reward: -1.3 \n",
      "episode: 49, loss: 19.242563247680664, reward: -1.97 \n",
      "episode: 50, loss: 19.362030029296875, reward: -2.46 \n",
      "episode: 51, loss: 19.783123016357422, reward: -2.53 \n",
      "episode: 52, loss: 19.4870548248291, reward: -1.92 \n",
      "episode: 53, loss: 19.323291778564453, reward: -2.98 \n",
      "episode: 54, loss: 19.41466522216797, reward: -1.95 \n",
      "episode: 55, loss: 19.78407096862793, reward: -3.1 \n",
      "episode: 56, loss: 19.07775115966797, reward: -2.58 \n",
      "episode: 57, loss: 19.74464988708496, reward: -2.36 \n",
      "episode: 58, loss: 19.438554763793945, reward: -1.97 \n",
      "episode: 59, loss: 19.364656448364258, reward: -2.52 \n",
      "episode: 60, loss: 19.331968307495117, reward: -1.97 \n",
      "episode: 61, loss: 19.718355178833008, reward: -2.31 \n",
      "episode: 62, loss: 19.888471603393555, reward: -2.45 \n",
      "episode: 63, loss: 19.98644256591797, reward: -1.3 \n",
      "episode: 64, loss: 19.782752990722656, reward: -2.61 \n",
      "episode: 65, loss: 19.752168655395508, reward: -1.32 \n",
      "episode: 66, loss: 19.261201858520508, reward: -2.52 \n",
      "episode: 67, loss: 19.72748374938965, reward: -1.96 \n",
      "episode: 68, loss: 19.769886016845703, reward: -3.05 \n",
      "episode: 69, loss: 19.903173446655273, reward: -2.77 \n",
      "episode: 70, loss: 19.981969833374023, reward: -2.58 \n",
      "episode: 71, loss: 19.82694435119629, reward: -2.38 \n",
      "episode: 72, loss: 19.814441680908203, reward: -2.52 \n",
      "episode: 73, loss: 19.851261138916016, reward: -3.95 \n",
      "episode: 74, loss: 20.132450103759766, reward: -2.8 \n",
      "episode: 75, loss: 20.30576515197754, reward: -1.92 \n",
      "episode: 76, loss: 20.184795379638672, reward: -2.46 \n",
      "episode: 77, loss: 19.711681365966797, reward: -1.94 \n",
      "episode: 78, loss: 19.977392196655273, reward: -1.32 \n",
      "episode: 79, loss: 19.768571853637695, reward: -3.11 \n",
      "episode: 80, loss: 19.805282592773438, reward: -1.96 \n",
      "episode: 81, loss: 19.70755958557129, reward: -1.96 \n",
      "episode: 82, loss: 19.331674575805664, reward: -1.36 \n",
      "episode: 83, loss: 19.55760383605957, reward: -2.47 \n",
      "episode: 84, loss: 19.985666275024414, reward: -1.96 \n",
      "episode: 85, loss: 20.545459747314453, reward: -3.11 \n",
      "episode: 86, loss: 19.916078567504883, reward: -1.84 \n",
      "episode: 87, loss: 20.439409255981445, reward: -3.0 \n",
      "episode: 88, loss: 20.146774291992188, reward: -1.32 \n",
      "episode: 89, loss: 20.722766876220703, reward: -2.88 \n",
      "episode: 90, loss: 19.618362426757812, reward: -1.96 \n",
      "episode: 91, loss: 19.837051391601562, reward: -1.97 \n",
      "episode: 92, loss: 20.26247787475586, reward: -2.48 \n",
      "episode: 93, loss: 20.411745071411133, reward: -1.97 \n",
      "episode: 94, loss: 20.527212142944336, reward: -1.96 \n",
      "episode: 95, loss: 19.737001419067383, reward: -2.55 \n",
      "episode: 96, loss: 20.01104736328125, reward: -2.5 \n",
      "episode: 97, loss: 19.836328506469727, reward: -2.54 \n",
      "episode: 98, loss: 20.218578338623047, reward: -2.51 \n",
      "episode: 99, loss: 20.677732467651367, reward: -1.94 \n",
      "episode: 100, loss: 19.76679229736328, reward: -2.51 \n",
      "episode: 101, loss: 19.793062210083008, reward: -2.5 \n",
      "episode: 102, loss: 20.45241355895996, reward: -1.96 \n",
      "episode: 103, loss: 20.51217269897461, reward: -2.54 \n",
      "episode: 104, loss: 20.10771369934082, reward: -1.32 \n",
      "episode: 105, loss: 19.563562393188477, reward: -1.34 \n",
      "episode: 106, loss: 19.76995277404785, reward: -1.93 \n",
      "episode: 107, loss: 19.505657196044922, reward: -1.94 \n",
      "episode: 108, loss: 19.902334213256836, reward: -2.82 \n",
      "episode: 109, loss: 19.552099227905273, reward: -2.51 \n",
      "episode: 110, loss: 19.86368179321289, reward: -2.52 \n",
      "episode: 111, loss: 20.409982681274414, reward: -2.56 \n",
      "episode: 112, loss: 19.615020751953125, reward: -2.61 \n",
      "episode: 113, loss: 19.951068878173828, reward: -1.95 \n",
      "episode: 114, loss: 19.922138214111328, reward: -1.31 \n",
      "episode: 115, loss: 19.54224395751953, reward: -2.46 \n",
      "episode: 116, loss: 19.95629119873047, reward: -1.3 \n",
      "episode: 117, loss: 19.296680450439453, reward: -2.92 \n",
      "episode: 118, loss: 19.9088134765625, reward: -2.46 \n",
      "episode: 119, loss: 19.946792602539062, reward: -1.96 \n",
      "episode: 120, loss: 19.94525146484375, reward: -1.3 \n",
      "episode: 121, loss: 19.860206604003906, reward: -2.53 \n",
      "episode: 122, loss: 19.1394100189209, reward: -1.68 \n",
      "episode: 123, loss: 18.7613582611084, reward: -2.48 \n",
      "episode: 124, loss: 18.534147262573242, reward: -3.02 \n",
      "episode: 125, loss: 18.928409576416016, reward: -2.55 \n",
      "episode: 126, loss: 18.914867401123047, reward: -3.09 \n",
      "episode: 127, loss: 19.903600692749023, reward: -1.33 \n",
      "episode: 128, loss: 19.12358856201172, reward: -1.95 \n",
      "episode: 129, loss: 19.01736068725586, reward: -1.32 \n",
      "episode: 130, loss: 19.016855239868164, reward: -1.93 \n",
      "episode: 131, loss: 18.786890029907227, reward: -1.57 \n",
      "episode: 132, loss: 18.789701461791992, reward: -1.96 \n",
      "episode: 133, loss: 19.09943199157715, reward: -1.92 \n",
      "episode: 134, loss: 18.471542358398438, reward: -1.95 \n",
      "episode: 135, loss: 19.18632698059082, reward: -2.51 \n",
      "episode: 136, loss: 18.960582733154297, reward: -1.98 \n",
      "episode: 137, loss: 18.924898147583008, reward: -2.47 \n",
      "episode: 138, loss: 18.802743911743164, reward: -1.94 \n",
      "episode: 139, loss: 18.998334884643555, reward: -1.33 \n",
      "episode: 140, loss: 18.86480712890625, reward: -2.55 \n",
      "episode: 141, loss: 18.675634384155273, reward: -2.95 \n",
      "episode: 142, loss: 18.902454376220703, reward: -1.95 \n",
      "episode: 143, loss: 18.678197860717773, reward: -2.53 \n",
      "episode: 144, loss: 18.822118759155273, reward: -1.92 \n",
      "episode: 145, loss: 18.101469039916992, reward: -1.9 \n",
      "episode: 146, loss: 18.955961227416992, reward: -1.42 \n",
      "episode: 147, loss: 18.43869400024414, reward: -2.46 \n",
      "episode: 148, loss: 18.829992294311523, reward: -1.34 \n",
      "episode: 149, loss: 18.205224990844727, reward: -3.12 \n",
      "episode: 150, loss: 18.41211700439453, reward: -1.55 \n",
      "episode: 151, loss: 18.50469398498535, reward: -2.46 \n",
      "episode: 152, loss: 18.268857955932617, reward: -2.46 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 153, loss: 18.4176025390625, reward: -2.55 \n",
      "episode: 154, loss: 18.60626220703125, reward: -3.07 \n",
      "episode: 155, loss: 18.93350601196289, reward: -2.49 \n",
      "episode: 156, loss: 19.124292373657227, reward: -3.09 \n",
      "episode: 157, loss: 18.795120239257812, reward: -1.34 \n",
      "episode: 158, loss: 19.037273406982422, reward: -1.32 \n",
      "episode: 159, loss: 18.968433380126953, reward: -1.33 \n",
      "episode: 160, loss: 18.27519416809082, reward: -1.94 \n",
      "episode: 161, loss: 18.481822967529297, reward: -3.07 \n",
      "episode: 162, loss: 18.646713256835938, reward: -2.51 \n",
      "episode: 163, loss: 18.661529541015625, reward: -3.14 \n",
      "episode: 164, loss: 19.20785140991211, reward: -2.61 \n",
      "episode: 165, loss: 19.069046020507812, reward: -1.95 \n",
      "episode: 166, loss: 18.520952224731445, reward: -2.0 \n",
      "episode: 167, loss: 18.851089477539062, reward: -2.54 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17144\\1133708682.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtransition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mepisode_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\EPFL\\ANN\\project-DDPG\\qnetwork2.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, transition, trunc, gamma)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m# not sure about this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[0mnext_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\EPFL\\ANN\\project-DDPG\\heuristic.py\u001b[0m in \u001b[0;36mcompute_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# because it is related to the theta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    646\u001b[0m                           \u001b[1;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                           'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(500): \n",
    "    state, info = norm_env.reset()\n",
    "    trunc = False\n",
    "    \n",
    "    episode_loss = []\n",
    "    av_episode_loss = 0\n",
    "    \n",
    "    episode_reward = []\n",
    "    av_episode_reward = 0\n",
    "    \n",
    "    while not trunc:\n",
    "        action = agent.compute_action(state)\n",
    "        # print(norm_env.step(action))\n",
    "        next_state, reward, terminated, trunc, info = norm_env.step(action)\n",
    "        buffer.add(state, action, reward, next_state, trunc)\n",
    "        \n",
    "        if len(buffer) > batch_size:\n",
    "            transition = buffer.sample(batch_size)\n",
    "            loss = critic.update(transition, trunc, 0.99)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            episode_loss.append(loss)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward.append(reward)\n",
    "        \n",
    "        if trunc:\n",
    "            av_episode_loss = np.mean(episode_loss)\n",
    "            av_episode_reward = np.mean(episode_reward)\n",
    "            sys.stdout.write(\"episode: {}, loss: {}, reward: {} \\n\".format(episode, av_episode_loss, np.round(av_episode_reward, decimals=2)))\n",
    "            break\n",
    "            \n",
    "    losses.append(av_episode_loss)\n",
    "    rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc039f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
