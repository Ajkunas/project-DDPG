{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79fc81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "from qnetwork2 import ReplayBuffer, QNetwork\n",
    "from heuristic import HeuristicPendulumAgent\n",
    "from matplotlib import pyplot\n",
    "import torch.optim as optim\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0dec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "norm_env = NormalizedEnv(env) # accept actions between -1 and 1\n",
    "\n",
    "#we fix a torque\n",
    "torque = norm_env.action(norm_env.action_space.sample())\n",
    "agent = HeuristicPendulumAgent(norm_env, torque)\n",
    "\n",
    "buffer = ReplayBuffer(10000)\n",
    "batch_size = 128\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "hidden_size = 256 # choose as you wish \n",
    "critic = QNetwork(num_states + num_actions, hidden_size, num_actions, agent)\n",
    "optimizer = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "\n",
    "losses = []\n",
    "rewards = []\n",
    "avg_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03fbc883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajkuna Seipi\\OneDrive\\Documents\\EPFL\\ANN\\project-DDPG\\qnetwork2.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\Users\\builder\\tkoch\\workspace\\pytorch\\pytorch_1647970138273\\work\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  states = torch.FloatTensor(states)\n",
      "C:\\Users\\Ajkuna Seipi\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Ajkuna Seipi\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, loss: 6139.49658203125, reward: -1838.28, average _reward: nan \n",
      "episode: 1, loss: 16785.60546875, reward: -1833.15, average _reward: -1838.27585434772 \n",
      "episode: 2, loss: 17376.490234375, reward: -1914.23, average _reward: -1835.7151080675276 \n",
      "episode: 3, loss: 17351.92578125, reward: -1815.39, average _reward: -1861.885110247916 \n",
      "episode: 4, loss: 16947.1328125, reward: -1626.89, average _reward: -1850.2605661189125 \n",
      "episode: 5, loss: 17034.421875, reward: -1897.82, average _reward: -1805.585911824471 \n",
      "episode: 6, loss: 17098.322265625, reward: -1831.43, average _reward: -1820.9586071058472 \n",
      "episode: 7, loss: 17384.103515625, reward: -1941.13, average _reward: -1822.4544551925242 \n",
      "episode: 8, loss: 17307.041015625, reward: -1830.2, average _reward: -1837.2883373137652 \n",
      "episode: 9, loss: 17319.98046875, reward: -1866.34, average _reward: -1836.5003860611696 \n",
      "episode: 10, loss: 17296.51953125, reward: -1732.13, average _reward: -1839.4840883237862 \n",
      "episode: 11, loss: 17135.46484375, reward: -1777.64, average _reward: -1828.869669033947 \n",
      "episode: 12, loss: 17185.494140625, reward: -1884.29, average _reward: -1823.3179666236247 \n",
      "episode: 13, loss: 17182.984375, reward: -1844.73, average _reward: -1820.32460047136 \n",
      "episode: 14, loss: 17329.708984375, reward: -1939.29, average _reward: -1823.258605230444 \n",
      "episode: 15, loss: 17028.82421875, reward: -989.97, average _reward: -1854.499363219497 \n",
      "episode: 16, loss: 16847.248046875, reward: -1894.27, average _reward: -1763.7143917582994 \n",
      "episode: 17, loss: 17001.25, reward: -1944.09, average _reward: -1769.998783260348 \n",
      "episode: 18, loss: 16942.73046875, reward: -1868.1, average _reward: -1770.2953500523206 \n",
      "episode: 19, loss: 16969.224609375, reward: -1769.37, average _reward: -1774.0856923813074 \n",
      "episode: 20, loss: 17041.009765625, reward: -1916.57, average _reward: -1764.388837412475 \n",
      "episode: 21, loss: 16895.158203125, reward: -1339.65, average _reward: -1782.8330860708083 \n",
      "episode: 22, loss: 16858.943359375, reward: -1874.84, average _reward: -1739.0342248295733 \n",
      "episode: 23, loss: 16864.01171875, reward: -1760.33, average _reward: -1738.0886784409176 \n",
      "episode: 24, loss: 16836.712890625, reward: -1873.87, average _reward: -1729.6492779211287 \n",
      "episode: 25, loss: 16934.830078125, reward: -1906.97, average _reward: -1723.1063680215073 \n",
      "episode: 26, loss: 16936.115234375, reward: -1854.46, average _reward: -1814.8065821885793 \n",
      "episode: 27, loss: 16949.203125, reward: -1879.73, average _reward: -1810.8255436003303 \n",
      "episode: 28, loss: 16979.056640625, reward: -1803.63, average _reward: -1804.3899152013284 \n",
      "episode: 29, loss: 16983.787109375, reward: -1916.94, average _reward: -1797.943293156722 \n",
      "episode: 30, loss: 17030.5625, reward: -1832.1, average _reward: -1812.7003645322152 \n",
      "episode: 31, loss: 16991.203125, reward: -1850.39, average _reward: -1804.2533082684138 \n",
      "episode: 32, loss: 17096.08984375, reward: -1900.12, average _reward: -1855.3271538733068 \n",
      "episode: 33, loss: 17072.0703125, reward: -1894.96, average _reward: -1857.8553526921492 \n",
      "episode: 34, loss: 17055.6796875, reward: -1523.83, average _reward: -1871.3180255634823 \n",
      "episode: 35, loss: 17009.4765625, reward: -1892.1, average _reward: -1836.3141276080605 \n",
      "episode: 36, loss: 17069.3984375, reward: -1792.71, average _reward: -1834.826557994671 \n",
      "episode: 37, loss: 16979.0703125, reward: -1757.8, average _reward: -1828.651525357202 \n",
      "episode: 38, loss: 16971.439453125, reward: -1530.33, average _reward: -1816.4577829023397 \n",
      "episode: 39, loss: 16944.935546875, reward: -1904.68, average _reward: -1789.1271065127348 \n",
      "episode: 40, loss: 16990.0234375, reward: -1813.56, average _reward: -1787.9014122829071 \n",
      "episode: 41, loss: 16981.517578125, reward: -1882.77, average _reward: -1786.0472005299168 \n",
      "episode: 42, loss: 17058.998046875, reward: -1886.74, average _reward: -1789.2856214510962 \n",
      "episode: 43, loss: 17065.560546875, reward: -1904.97, average _reward: -1787.948183004864 \n",
      "episode: 44, loss: 17020.349609375, reward: -1889.77, average _reward: -1788.9487875926302 \n",
      "episode: 45, loss: 17081.451171875, reward: -1833.92, average _reward: -1825.5430200754558 \n",
      "episode: 46, loss: 17026.576171875, reward: -1827.15, average _reward: -1819.7253369655493 \n",
      "episode: 47, loss: 17100.47265625, reward: -1814.58, average _reward: -1823.168575300199 \n",
      "episode: 48, loss: 17077.513671875, reward: -1900.27, average _reward: -1828.8471833981107 \n",
      "episode: 49, loss: 17052.9921875, reward: -1906.88, average _reward: -1865.8412038155936 \n",
      "episode: 50, loss: 17141.52734375, reward: -1912.03, average _reward: -1866.0606712447684 \n",
      "episode: 51, loss: 17073.55078125, reward: -1520.83, average _reward: -1875.9071785314736 \n",
      "episode: 52, loss: 17039.3046875, reward: -1697.78, average _reward: -1839.713075684906 \n",
      "episode: 53, loss: 17008.833984375, reward: -1661.1, average _reward: -1820.8162392336405 \n",
      "episode: 54, loss: 17095.685546875, reward: -1849.08, average _reward: -1796.4293348392264 \n",
      "episode: 55, loss: 17022.748046875, reward: -1932.87, average _reward: -1792.3602036759573 \n",
      "episode: 56, loss: 17039.275390625, reward: -1633.64, average _reward: -1802.2546644515903 \n",
      "episode: 57, loss: 16953.79296875, reward: -1842.86, average _reward: -1782.9041817513712 \n",
      "episode: 58, loss: 17016.0390625, reward: -1897.44, average _reward: -1785.7315151146108 \n",
      "episode: 59, loss: 16938.8671875, reward: -1480.95, average _reward: -1785.4484434511244 \n",
      "episode: 60, loss: 16934.9609375, reward: -1846.25, average _reward: -1742.8555912963554 \n",
      "episode: 61, loss: 16979.28125, reward: -1853.35, average _reward: -1736.2774991394563 \n",
      "episode: 62, loss: 16963.28125, reward: -1763.94, average _reward: -1769.52973328311 \n",
      "episode: 63, loss: 16981.134765625, reward: -1929.99, average _reward: -1776.1460692651267 \n",
      "episode: 64, loss: 16912.861328125, reward: -1908.74, average _reward: -1803.035567399675 \n",
      "episode: 65, loss: 17094.98046875, reward: -1797.47, average _reward: -1809.001854950036 \n",
      "episode: 66, loss: 17078.97265625, reward: -1908.94, average _reward: -1795.4622634077768 \n",
      "episode: 67, loss: 17113.310546875, reward: -1871.4, average _reward: -1822.9922740642814 \n",
      "episode: 68, loss: 17083.048828125, reward: -1800.23, average _reward: -1825.8464434165842 \n",
      "episode: 69, loss: 17123.66796875, reward: -1851.74, average _reward: -1816.1261300399733 \n",
      "episode: 70, loss: 17067.623046875, reward: -1904.61, average _reward: -1853.2053123270725 \n",
      "episode: 71, loss: 17145.62890625, reward: -1849.76, average _reward: -1859.0418172138784 \n",
      "episode: 72, loss: 17174.568359375, reward: -1825.69, average _reward: -1858.6822054035133 \n",
      "episode: 73, loss: 17229.013671875, reward: -1865.2, average _reward: -1864.8570809863236 \n",
      "episode: 74, loss: 17190.51171875, reward: -1846.8, average _reward: -1858.3782239865068 \n",
      "episode: 75, loss: 17186.341796875, reward: -1833.74, average _reward: -1852.183676283923 \n",
      "episode: 76, loss: 17205.625, reward: -1948.02, average _reward: -1855.8106911194895 \n",
      "episode: 77, loss: 17221.263671875, reward: -1854.23, average _reward: -1859.7182983577684 \n",
      "episode: 78, loss: 17227.935546875, reward: -1678.47, average _reward: -1858.0015451189079 \n",
      "episode: 79, loss: 17104.4453125, reward: -1869.04, average _reward: -1845.8247810384314 \n",
      "episode: 80, loss: 17198.380859375, reward: -1905.89, average _reward: -1847.5549791420094 \n",
      "episode: 81, loss: 17224.451171875, reward: -1903.02, average _reward: -1847.682991841452 \n",
      "episode: 82, loss: 17164.53515625, reward: -1883.42, average _reward: -1853.0096157682779 \n",
      "episode: 83, loss: 17203.87109375, reward: -1822.73, average _reward: -1858.7833386114453 \n",
      "episode: 84, loss: 17142.73828125, reward: -1596.63, average _reward: -1854.5361589965523 \n",
      "episode: 85, loss: 17112.05859375, reward: -1759.83, average _reward: -1829.5193313462537 \n",
      "episode: 86, loss: 17095.0390625, reward: -1735.51, average _reward: -1822.1286116543447 \n",
      "episode: 87, loss: 17149.732421875, reward: -1634.14, average _reward: -1800.878107474205 \n",
      "episode: 88, loss: 17137.189453125, reward: -1828.76, average _reward: -1778.8685164773265 \n",
      "episode: 89, loss: 17151.84375, reward: -1828.95, average _reward: -1793.8981344658246 \n",
      "episode: 90, loss: 17161.224609375, reward: -1534.99, average _reward: -1789.8887694127486 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 91, loss: 17086.263671875, reward: -1926.19, average _reward: -1752.7983381917431 \n",
      "episode: 92, loss: 17132.609375, reward: -1934.17, average _reward: -1755.1151631366308 \n",
      "episode: 93, loss: 17084.341796875, reward: -1842.87, average _reward: -1760.1899358432813 \n",
      "episode: 94, loss: 17135.35546875, reward: -1812.82, average _reward: -1762.2041045854226 \n",
      "episode: 95, loss: 17100.6953125, reward: -1912.44, average _reward: -1783.8237217729202 \n",
      "episode: 96, loss: 17085.31640625, reward: -1735.4, average _reward: -1799.0847570623864 \n",
      "episode: 97, loss: 17117.76953125, reward: -1927.22, average _reward: -1799.0737280501428 \n",
      "episode: 98, loss: 17116.951171875, reward: -1807.51, average _reward: -1828.3827075817376 \n",
      "episode: 99, loss: 17056.07421875, reward: -1885.52, average _reward: -1826.2573686667045 \n",
      "episode: 100, loss: 17128.919921875, reward: -1930.09, average _reward: -1831.9140416140403 \n",
      "episode: 101, loss: 17216.171875, reward: -1943.12, average _reward: -1871.4243408165662 \n",
      "episode: 102, loss: 17228.97265625, reward: -1917.81, average _reward: -1873.1169563145443 \n",
      "episode: 103, loss: 17281.033203125, reward: -1779.11, average _reward: -1871.4811781997537 \n",
      "episode: 104, loss: 17312.443359375, reward: -1910.03, average _reward: -1865.1051866656737 \n",
      "episode: 105, loss: 17351.77734375, reward: -1899.69, average _reward: -1874.8259913752347 \n",
      "episode: 106, loss: 17308.6796875, reward: -1839.57, average _reward: -1873.5504211664222 \n",
      "episode: 107, loss: 17353.484375, reward: -1912.33, average _reward: -1883.9670231343152 \n",
      "episode: 108, loss: 17245.015625, reward: -1542.7, average _reward: -1882.4778387269193 \n",
      "episode: 109, loss: 17273.96875, reward: -1236.91, average _reward: -1855.9971183594996 \n",
      "episode: 110, loss: 17236.802734375, reward: -1913.08, average _reward: -1791.1369395410588 \n",
      "episode: 111, loss: 17224.455078125, reward: -1788.77, average _reward: -1789.4356920809132 \n",
      "episode: 112, loss: 17247.40625, reward: -1914.84, average _reward: -1774.000967529723 \n",
      "episode: 113, loss: 17291.93359375, reward: -1838.85, average _reward: -1773.703424502083 \n",
      "episode: 114, loss: 17188.337890625, reward: -1910.74, average _reward: -1779.6768564917206 \n",
      "episode: 115, loss: 17304.83203125, reward: -1871.39, average _reward: -1779.7474868927238 \n",
      "episode: 116, loss: 17256.279296875, reward: -1748.51, average _reward: -1776.9177558286253 \n",
      "episode: 117, loss: 17219.2890625, reward: -1804.16, average _reward: -1767.8119661357625 \n",
      "episode: 118, loss: 17222.11328125, reward: -1889.55, average _reward: -1756.9948119049764 \n",
      "episode: 119, loss: 17237.4140625, reward: -1800.35, average _reward: -1791.6798958782433 \n",
      "episode: 120, loss: 17232.341796875, reward: -1725.66, average _reward: -1848.023234914846 \n",
      "episode: 121, loss: 17207.65234375, reward: -1898.24, average _reward: -1829.2815425145018 \n",
      "episode: 122, loss: 17214.564453125, reward: -1909.28, average _reward: -1840.228682016243 \n",
      "episode: 123, loss: 17098.173828125, reward: -1405.3, average _reward: -1839.6722770855008 \n",
      "episode: 124, loss: 17108.990234375, reward: -1862.6, average _reward: -1796.3180345881744 \n",
      "episode: 125, loss: 17063.037109375, reward: -1475.58, average _reward: -1791.5043340209359 \n",
      "episode: 126, loss: 16983.048828125, reward: -1906.84, average _reward: -1751.9230217501986 \n",
      "episode: 127, loss: 17091.96484375, reward: -1904.49, average _reward: -1767.756409258005 \n",
      "episode: 128, loss: 17110.208984375, reward: -1934.6, average _reward: -1777.7887805519101 \n",
      "episode: 129, loss: 17147.3203125, reward: -1909.05, average _reward: -1782.29339613044 \n",
      "episode: 130, loss: 17064.431640625, reward: -1731.25, average _reward: -1793.1639778856509 \n",
      "episode: 131, loss: 17073.99609375, reward: -1911.65, average _reward: -1793.7231677364193 \n",
      "episode: 132, loss: 17117.564453125, reward: -1905.98, average _reward: -1795.0636713530469 \n",
      "episode: 133, loss: 17050.958984375, reward: -1738.06, average _reward: -1794.734563788282 \n",
      "episode: 134, loss: 17186.318359375, reward: -1921.61, average _reward: -1828.0104166784718 \n",
      "episode: 135, loss: 17147.791015625, reward: -1848.0, average _reward: -1833.911042230319 \n",
      "episode: 136, loss: 17228.671875, reward: -1844.09, average _reward: -1871.1528069941348 \n",
      "episode: 137, loss: 17238.119140625, reward: -1728.73, average _reward: -1864.8776086376208 \n",
      "episode: 138, loss: 17248.365234375, reward: -1846.61, average _reward: -1847.3019928203753 \n",
      "episode: 139, loss: 17104.5859375, reward: -1507.97, average _reward: -1838.5032256981638 \n",
      "episode: 140, loss: 17183.7109375, reward: -1711.03, average _reward: -1798.3945131269734 \n",
      "episode: 141, loss: 17176.642578125, reward: -1902.31, average _reward: -1796.3726583262237 \n",
      "episode: 142, loss: 17165.84375, reward: -1838.07, average _reward: -1795.4390973828322 \n",
      "episode: 143, loss: 17160.53125, reward: -1906.01, average _reward: -1788.6473083620124 \n",
      "episode: 144, loss: 17205.94140625, reward: -1853.74, average _reward: -1805.4417609206416 \n",
      "episode: 145, loss: 17108.74609375, reward: -1579.86, average _reward: -1798.6553108027151 \n",
      "episode: 146, loss: 17108.517578125, reward: -1853.86, average _reward: -1771.841301236691 \n",
      "episode: 147, loss: 17090.46875, reward: -1930.26, average _reward: -1772.817854346477 \n",
      "episode: 148, loss: 17109.279296875, reward: -1912.26, average _reward: -1792.970672619826 \n",
      "episode: 149, loss: 17156.154296875, reward: -1874.72, average _reward: -1799.5356166210015 \n",
      "episode: 150, loss: 17119.978515625, reward: -1957.85, average _reward: -1836.2114231627045 \n",
      "episode: 151, loss: 17127.568359375, reward: -1856.51, average _reward: -1860.893027960455 \n",
      "episode: 152, loss: 17074.646484375, reward: -1660.5, average _reward: -1856.3126795294145 \n",
      "episode: 153, loss: 17072.080078125, reward: -1836.39, average _reward: -1838.556443427471 \n",
      "episode: 154, loss: 17082.482421875, reward: -1893.27, average _reward: -1831.5943169791954 \n",
      "episode: 155, loss: 17017.611328125, reward: -1892.45, average _reward: -1835.5473051103938 \n",
      "episode: 156, loss: 17132.607421875, reward: -1847.65, average _reward: -1866.806306754562 \n",
      "episode: 157, loss: 17057.92578125, reward: -1854.96, average _reward: -1866.1860685424945 \n",
      "episode: 158, loss: 17120.064453125, reward: -1865.42, average _reward: -1858.6564310384674 \n",
      "episode: 159, loss: 17213.162109375, reward: -1755.92, average _reward: -1853.9724329986268 \n",
      "episode: 160, loss: 17224.572265625, reward: -1825.76, average _reward: -1842.092009612994 \n",
      "episode: 161, loss: 17210.740234375, reward: -1810.83, average _reward: -1828.883318879062 \n",
      "episode: 162, loss: 17225.421875, reward: -1824.22, average _reward: -1824.3158233645565 \n",
      "episode: 163, loss: 17180.611328125, reward: -1742.94, average _reward: -1840.6877603773294 \n",
      "episode: 164, loss: 17156.7578125, reward: -1750.76, average _reward: -1831.3433306469608 \n",
      "episode: 165, loss: 17088.30859375, reward: -1874.43, average _reward: -1817.0917896621172 \n",
      "episode: 166, loss: 17117.681640625, reward: -1788.73, average _reward: -1815.2898666295719 \n",
      "episode: 167, loss: 17189.0703125, reward: -1910.05, average _reward: -1809.3978098045295 \n",
      "episode: 168, loss: 17143.59375, reward: -1660.34, average _reward: -1814.9065424179862 \n",
      "episode: 169, loss: 17171.45703125, reward: -1858.01, average _reward: -1794.3984935416713 \n",
      "episode: 170, loss: 17138.537109375, reward: -1608.43, average _reward: -1804.6072220477495 \n",
      "episode: 171, loss: 17104.783203125, reward: -1905.09, average _reward: -1782.8739743695319 \n",
      "episode: 172, loss: 17138.017578125, reward: -1900.36, average _reward: -1792.3002060666277 \n",
      "episode: 173, loss: 17133.22265625, reward: -1260.59, average _reward: -1799.9143414508756 \n",
      "episode: 174, loss: 17100.15234375, reward: -1912.33, average _reward: -1751.679153068369 \n",
      "episode: 175, loss: 17054.45703125, reward: -1746.12, average _reward: -1767.836755618003 \n",
      "episode: 176, loss: 17184.30078125, reward: -1916.32, average _reward: -1755.0059746864822 \n",
      "episode: 177, loss: 17140.0, reward: -1749.37, average _reward: -1767.7650116252782 \n",
      "episode: 178, loss: 17095.44140625, reward: -1794.32, average _reward: -1751.6971040137603 \n",
      "episode: 179, loss: 17027.25, reward: -1607.42, average _reward: -1765.0953124657728 \n",
      "episode: 180, loss: 17051.431640625, reward: -1827.34, average _reward: -1740.0364554644416 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 181, loss: 17103.5, reward: -1898.76, average _reward: -1761.9273166842552 \n",
      "episode: 182, loss: 17011.984375, reward: -1824.94, average _reward: -1761.2938153643695 \n",
      "episode: 183, loss: 17063.6796875, reward: -1943.25, average _reward: -1753.751777782683 \n",
      "episode: 184, loss: 17143.072265625, reward: -1923.57, average _reward: -1822.0178740244482 \n",
      "episode: 185, loss: 17051.1640625, reward: -1847.71, average _reward: -1823.1411667805507 \n",
      "episode: 186, loss: 17141.154296875, reward: -1951.3, average _reward: -1833.3000239282828 \n",
      "episode: 187, loss: 17147.595703125, reward: -1943.63, average _reward: -1836.7978546136942 \n",
      "episode: 188, loss: 17171.2265625, reward: -1859.9, average _reward: -1856.223990962118 \n",
      "episode: 189, loss: 17208.71875, reward: -1566.65, average _reward: -1862.7821986368358 \n",
      "episode: 190, loss: 17135.501953125, reward: -1807.38, average _reward: -1858.7049087117496 \n",
      "episode: 191, loss: 17185.310546875, reward: -1920.53, average _reward: -1856.7087999027954 \n",
      "episode: 192, loss: 17243.166015625, reward: -1905.91, average _reward: -1858.8863007210725 \n",
      "episode: 193, loss: 17221.228515625, reward: -1771.3, average _reward: -1866.9824029744555 \n",
      "episode: 194, loss: 17174.21484375, reward: -1872.58, average _reward: -1849.7868700131871 \n",
      "episode: 195, loss: 17329.091796875, reward: -1931.22, average _reward: -1844.6887041673788 \n",
      "episode: 196, loss: 17257.529296875, reward: -1822.34, average _reward: -1853.039641652466 \n",
      "episode: 197, loss: 17270.810546875, reward: -1889.06, average _reward: -1840.1436491117263 \n",
      "episode: 198, loss: 17166.943359375, reward: -1283.71, average _reward: -1834.6865717423557 \n",
      "episode: 199, loss: 17121.638671875, reward: -1792.99, average _reward: -1777.0667811818407 \n"
     ]
    }
   ],
   "source": [
    "for episode in range(200): \n",
    "    state, info = norm_env.reset()\n",
    "    trunc = False\n",
    "    episode_loss = 0\n",
    "    #average_loss = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not trunc:\n",
    "        action = agent.compute_action(state)\n",
    "        # print(norm_env.step(action))\n",
    "        next_state, reward, terminated, trunc, info = norm_env.step(action)\n",
    "        buffer.add(state, action, reward, next_state, trunc)\n",
    "        \n",
    "        if len(buffer) > batch_size:\n",
    "            transition = buffer.sample(batch_size)\n",
    "            loss = critic.update(optimizer, transition, trunc, 0.99)\n",
    "            episode_loss += loss\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if trunc:\n",
    "           # average_loss = np.mean(episode_loss[-10:]) # average of loss \n",
    "            sys.stdout.write(\"episode: {}, loss: {}, reward: {}, average _reward: {} \\n\".format(episode, episode_loss, np.round(episode_reward, decimals=2), np.mean(rewards[-10:])))\n",
    "            break\n",
    "            \n",
    "    losses.append(episode_loss)\n",
    "    rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc039f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
