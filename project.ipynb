{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e31bf8",
   "metadata": {},
   "source": [
    "## Control in a continuous action space with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dabef",
   "metadata": {},
   "source": [
    "### Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94bfa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "from matplotlib import pyplot\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d2e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "norm_env = NormalizedEnv(env) # accept actions between -1 and 1\n",
    "\n",
    "rand_ag = RandomAgent(norm_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one episode with a defined number of steps\n",
    "\n",
    "def episode(agent): \n",
    "    state, info = norm_env.reset()\n",
    "    tot_reward = 0\n",
    "    truncated = False\n",
    "\n",
    "    while not truncated:\n",
    "        action = agent.compute_action(state)\n",
    "        next_state, reward, terminated, truncated, info = norm_env.step(action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        if truncated:\n",
    "            state, info = norm_env.reset()\n",
    "            \n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73172ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1289.6196920408427, -1542.2102992915043, -1413.0309438154127, -918.1516068433682, -862.1930697438868, -1101.0063537917836, -1346.0926994053716, -1514.9667587227898, -1300.0919531774455, -1077.0732295215737]\n",
      "average cumulative reward : -1236.443660635398\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes \n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(rand_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a heuristic policy for the pendulum\n",
    "    \n",
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        _, y, v = state\n",
    "        torque = env.m*env.g*env.l*y # fixed torque\n",
    "        action = np.empty((1,))\n",
    "        \n",
    "        if (y < 0):\n",
    "            np.append(action, np.sign(v)*torque) # same direction to angular velocity\n",
    "        else:\n",
    "            np.append(action, (-1)*np.sign(v)*torque) # opposite direction to angular velocity\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2726e186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1666.0455191434114, -1687.151799593187, -1681.0061164486185, -1666.8698794844715, -1388.1506374083795, -1654.700067899993, -1639.1654516194753, -1650.1731060060715, -1676.455174969775, -1668.7891495776178]\n",
      "average cumulative reward : -1637.8506902151003\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes with Heuristic agent\n",
    "heur_ag = HeuristicPendulumAgent(norm_env)\n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(heur_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09cb90",
   "metadata": {},
   "source": [
    "## QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e3dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        self.idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, trunc):\n",
    "        transition = (state, action, reward, next_state, trunc)\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            # not sure about the behaviour when buffer is overloaded.\n",
    "            self.buffer[self.idx] = transition\n",
    "            self.idx = (self.idx + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, truncs = zip(*batch)\n",
    "        return states, actions, rewards, next_states, truncs\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, agent, norm_env):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.agent = agent\n",
    "        self.norm_env = norm_env\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def update(self, transition):\n",
    "        state = transition[:, :3]\n",
    "        action = transition[:, 3]\n",
    "        # Compute the TD target\n",
    "        with torch.no_grad():\n",
    "            targets = []\n",
    "            for s, a in zip(state, action):\n",
    "                next_state, reward, terminated, truncated, info = norm_env.step(a.numpy()) \n",
    "                next_actions = self.agent.compute_action(next_state)\n",
    "                \n",
    "                next_state, next_actions  = torch.Tensor(next_state).view(1, -1), torch.Tensor(next_actions).view(1, -1)\n",
    "                q_next = self.forward(torch.cat([next_state, next_actions], dim=1))\n",
    "                target = reward + gamma * q_next * (1 - truncated)\n",
    "                targets.append(target[0])  \n",
    "            targets = torch.Tensor(targets)\n",
    "            \n",
    "        q_values = self.forward(transition)\n",
    "        \n",
    "        loss = F.mse_loss(q_values.view(-1), targets)\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9fcc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1202743/3438573092.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  action = torch.Tensor([action])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "gamma = 0.01\n",
    "model = QNetwork(heur_ag, norm_env)\n",
    "\n",
    "state, info = norm_env.reset()\n",
    "\n",
    "action = heur_ag.compute_action(state)\n",
    "\n",
    "action = torch.Tensor([action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c87dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8912, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_t = torch.Tensor([state, state, state])\n",
    "action_t = torch.Tensor([action, action, action]).view(-1, 1)\n",
    "\n",
    "transition = torch.cat([state_t, action_t], dim=1)\n",
    "model.update(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252df2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.0153, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.update(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7136b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8e682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7735e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37812f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f8c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f185204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
