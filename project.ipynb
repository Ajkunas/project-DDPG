{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e31bf8",
   "metadata": {},
   "source": [
    "## Control in a continuous action space with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dabef",
   "metadata": {},
   "source": [
    "### Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94bfa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d2e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "norm_env = NormalizedEnv(env) # accept actions between -1 and 1\n",
    "\n",
    "rand_ag = RandomAgent(norm_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one episode with a defined number of steps\n",
    "\n",
    "def episode(agent): \n",
    "    state, info = norm_env.reset()\n",
    "    tot_reward = 0\n",
    "    truncated = False\n",
    "\n",
    "    while not truncated:\n",
    "        action = agent.compute_action(state)\n",
    "        next_state, reward, terminated, truncated, info = norm_env.step(action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        if truncated:\n",
    "            state, info = norm_env.reset()\n",
    "            \n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73172ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1034.1114585696762, -1284.5110112181655, -891.9493067719303, -1161.0561444634814, -1395.9371514588634, -1695.1204312216835, -1173.4723828199124, -1727.0671849252008, -1187.339686815846, -878.7176400864324]\n",
      "average cumulative reward : -1242.9282398351193\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes \n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(rand_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a heuristic policy for the pendulum\n",
    "\n",
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        _, y, v = state\n",
    "        torque = env.m*env.g*env.l*y # fixed torque\n",
    "        action = np.empty((1,))\n",
    "        \n",
    "        if (y < 0):\n",
    "            np.append(action, np.sign(v)*torque) # same direction to angular velocity\n",
    "        else:\n",
    "            np.append(action, (-1)*np.sign(v)*torque) # opposite direction to angular velocity\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2726e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1579.682712767844, -1683.7214117695607, -1322.168176344182, -845.5593601845148, -1664.4463129102007, -1676.7009767766742, -1678.420427318271, -1654.9513612868668, -1457.052430595924, -1682.3992653853645]\n",
      "average cumulative reward : -1524.5102435339402\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes with Heuristic agent\n",
    "heur_ag = HeuristicPendulumAgent(norm_env)\n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(heur_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5d111",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb135e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37812f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f8c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f185204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
