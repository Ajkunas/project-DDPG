{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e31bf8",
   "metadata": {},
   "source": [
    "## Control in a continuous action space with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dabef",
   "metadata": {},
   "source": [
    "### Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94bfa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38d2e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "norm_env = NormalizedEnv(env) # accept actions between -1 and 1\n",
    "\n",
    "rand_ag = RandomAgent(norm_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db1613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one episode with a defined number of steps\n",
    "\n",
    "def episode(agent): \n",
    "    state, info = norm_env.reset()\n",
    "    tot_reward = 0\n",
    "    truncated = False\n",
    "\n",
    "    while not truncated:\n",
    "        action = agent.compute_action(state)\n",
    "        next_state, reward, terminated, truncated, info = norm_env.step(action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        if truncated:\n",
    "            state, info = norm_env.reset()\n",
    "            \n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73172ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1163.5321072544002, -1167.216006332102, -974.2845367391204, -1387.3297895213423, -1347.7154263775685, -1685.8919005564503, -900.598325969867, -1182.7732059302364, -1538.2811571975667, -1642.764573246596]\n",
      "average cumulative reward : -1299.038702912525\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes \n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(rand_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a heuristic policy for the pendulum\n",
    "\n",
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        _, y, v = state\n",
    "        torque = env.m*env.g*env.l*y # fixed torque\n",
    "        action = np.empty((1,))\n",
    "        \n",
    "        if (y < 0):\n",
    "            np.append(action, np.sign(v)*torque) # same direction to angular velocity\n",
    "        else:\n",
    "            np.append(action, (-1)*np.sign(v)*torque) # opposite direction to angular velocity\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2726e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [-0.92779315 -0.37309507 -0.17099966]\n",
      "state: [-0.9411425  -0.33801004 -0.750821  ]\n",
      "reward: -7.620384162314936\n",
      "state: [-0.96116996 -0.27595705 -1.3043286 ]\n",
      "reward: -7.882412885646909\n",
      "state: [-0.98218876 -0.18789689 -1.8112963 ]\n",
      "reward: -8.365213557258404\n",
      "state: [-0.99708223 -0.07633518 -2.252219  ]\n",
      "reward: -9.049762017995807\n",
      "state: [-0.9985389   0.05403768 -2.6094704 ]\n",
      "reward: -9.906596747300354\n",
      "state: [-0.980558    0.19622944 -2.868942  ]\n",
      "reward: -10.21776666582613\n",
      "state: [-0.939852  0.341582 -3.02177 ]\n",
      "reward: -9.494698974723043\n",
      "state: [-0.87668025  0.4810735  -3.0655835 ]\n",
      "reward: -8.717919509136221\n",
      "state: [-0.7948004   0.60687095 -3.0047784 ]\n",
      "reward: -7.911869507749309\n",
      "state: [-0.700571   0.7135827 -2.849625 ]\n",
      "reward: -7.104354818653501\n",
      "state: [-0.6015783  0.7988138 -2.614438 ]\n",
      "reward: -6.324415734496672\n",
      "state: [-0.5052823   0.86295414 -2.315328  ]\n",
      "reward: -5.5993892796620806\n",
      "state: [-0.41805524  0.90842164 -1.9681122 ]\n",
      "reward: -4.952197157658203\n",
      "state: [-0.34474173  0.9386976  -1.586796  ]\n",
      "reward: -4.399750080760782\n",
      "state: [-0.28865814  0.9574322  -1.1827729 ]\n",
      "reward: -3.9527980825995437\n",
      "state: [-0.25184873  0.96776664 -0.7646986 ]\n",
      "reward: -3.6169796378811365\n",
      "state: [-0.23541582  0.97189474 -0.33887368]\n",
      "reward: -3.3945119243616\n",
      "state: [-0.23978925  0.97082496  0.09004737]\n",
      "reward: -3.28594860735941\n",
      "state: [-0.2648584   0.96428734  0.5181661 ]\n",
      "reward: -3.2915807464559483\n",
      "state: [-0.3099364   0.95075727  0.9413816 ]\n",
      "reward: -3.4122314130961273\n",
      "state: [-0.37356436  0.92760426  1.3544495 ]\n",
      "reward: -3.6493235031120737\n",
      "state: [-0.4532039  0.8914069  1.7501527]\n",
      "reward: -4.004181953004508\n",
      "state: [-0.5449183  0.8384892  2.118708 ]\n",
      "reward: -4.476607351698772\n",
      "state: [-0.64320016  0.7656981   2.4475749 ]\n",
      "reward: -5.062879131241558\n",
      "state: [-0.74113727  0.6713535   2.7218482 ]\n",
      "reward: -5.753541434300658\n",
      "state: [-0.8310712  0.556166   2.9253635]\n",
      "reward: -6.531560602626025\n",
      "state: [-0.90575397  0.42380393  3.0424879 ]\n",
      "reward: -7.37159775475196\n",
      "state: [-0.95976734  0.2807965   3.060341  ]\n",
      "reward: -8.241028145142494\n",
      "state: [-0.9907558   0.13565741  2.9709382 ]\n",
      "reward: -9.102839107540337\n",
      "state: [-9.9999672e-01 -2.5570791e-03  2.7726812e+00]\n",
      "reward: -9.91977185843713\n",
      "state: [-0.99206054 -0.12576137  2.4707634 ]\n",
      "reward: -10.626320503467241\n",
      "state: [-0.9736852  -0.22789708  2.0764425 ]\n",
      "reward: -9.707691783250887\n",
      "state: [-0.95227456 -0.30524278  1.6055197 ]\n",
      "reward: -8.913014224094153\n",
      "state: [-0.93447214 -0.35603625  1.0765876 ]\n",
      "reward: -8.27859125994604\n",
      "state: [-0.9250987  -0.37972662  0.5095604 ]\n",
      "reward: -7.834798662897625\n",
      "state: [-0.92652065 -0.37624398 -0.07523455]\n",
      "reward: -7.603974918450093\n",
      "state: [-0.93838537 -0.34559068 -0.65741754]\n",
      "reward: -7.599295150116486\n",
      "state: [-0.95765924 -0.28790417 -1.2166106 ]\n",
      "reward: -7.824204050377955\n",
      "state: [-0.9789773  -0.20396926 -1.7325387 ]\n",
      "reward: -8.271977900826505\n",
      "state: [-0.9953826  -0.09598692 -2.1855156 ]\n",
      "reward: -8.925333459521239\n",
      "state: [-0.99949634  0.03173504 -2.5575058 ]\n",
      "reward: -9.756460626622864\n",
      "state: [-0.9849994   0.17255756 -2.8337045 ]\n",
      "reward: -10.32926484615779\n",
      "state: [-0.9480841  0.3180196 -3.0042863]\n",
      "reward: -9.617003841450726\n",
      "state: [-0.8884091  0.4590526 -3.0657716]\n",
      "reward: -8.847431320285907\n",
      "state: [-0.8092027   0.58752966 -3.0214822 ]\n",
      "reward: -8.044330680206008\n",
      "state: [-0.71648604  0.69760144 -2.880835  ]\n",
      "reward: -7.235070423813633\n",
      "state: [-0.61774373  0.78637946 -2.657634  ]\n",
      "reward: -6.448680180942936\n",
      "state: [-0.52053535  0.8538401  -2.3678493 ]\n",
      "reward: -5.712979310153988\n",
      "state: [-0.43145442  0.9021347  -2.0274692 ]\n",
      "reward: -5.051756374162975\n",
      "state: [-0.35560468  0.9346365  -1.6508682 ]\n",
      "reward: -4.4829515015529795\n",
      "state: [-0.29653883  0.9550208  -1.2498908 ]\n",
      "reward: -4.018274633018453\n",
      "state: [-0.25648633  0.96654785 -0.8336252 ]\n",
      "reward: -3.6640927998732504\n",
      "state: [-0.23668204  0.9715872  -0.40871432]\n",
      "reward: -3.4230573270038858\n",
      "state: [-0.23765235  0.97135025  0.01997606]\n",
      "reward: -3.2958844498980926\n",
      "state: [-0.25937274  0.9657773   0.44848877]\n",
      "reward: -3.2828357723372226\n",
      "state: [-0.30126     0.95354205  0.87282175]\n",
      "reward: -3.3846228012982453\n",
      "state: [-0.36200014  0.932178    1.2879783 ]\n",
      "reward: -3.6025978819168327\n",
      "state: [-0.43925413  0.8983628   1.6871117 ]\n",
      "reward: -3.938181184733654\n",
      "state: [-0.5293315  0.8484151  2.060884 ]\n",
      "reward: -4.391546529139734\n",
      "state: [-0.6269813  0.7790343  2.3971953]\n",
      "reward: -4.959699621386944\n",
      "state: [-0.7254898   0.68823296  2.6814709 ]\n",
      "reward: -5.63426530934508\n",
      "state: [-0.81725305  0.576279    2.8976457 ]\n",
      "reward: -6.399535488773143\n",
      "state: [-0.8948615   0.44634393  3.029855  ]\n",
      "reward: -7.2315090337984245\n",
      "state: [-0.9525028   0.30452994  3.0646129 ]\n",
      "reward: -8.098599039014735\n",
      "state: [-0.98726004  0.1591152   2.9930103 ]\n",
      "reward: -8.964246174113889\n",
      "state: [-0.9998161   0.01917586  2.8123467 ]\n",
      "reward: -9.790932071928179\n",
      "state: [-0.99426395 -0.10695443  2.5267286 ]\n",
      "reward: -10.544408696109722\n",
      "state: [-0.9770861 -0.2128443  2.1465127]\n",
      "reward: -9.85022094377389\n",
      "state: [-0.95568186 -0.29440132  1.6868795 ]\n",
      "reward: -9.032710728003787\n",
      "state: [-0.93690294 -0.34958953  1.1660786 ]\n",
      "reward: -8.36986138509991\n",
      "state: [-0.9259219  -0.37771502  0.6038864 ]\n",
      "reward: -7.893189812478918\n",
      "state: [-0.92553234 -0.37866852  0.02060017]\n",
      "reward: -7.626445721314736\n",
      "state: [-0.93583083 -0.35244945 -0.5634012 ]\n",
      "reward: -7.5843475012016475\n",
      "state: [-0.9542065  -0.29914862 -1.1277384 ]\n",
      "reward: -7.771958328872807\n",
      "state: [-0.9756359  -0.21939597 -1.6520997 ]\n",
      "reward: -8.184244831791371\n",
      "state: [-0.99335307 -0.11510725 -2.1166468 ]\n",
      "reward: -8.805664023717897\n",
      "state: [-0.99995214  0.00978573 -2.5029771 ]\n",
      "reward: -9.610085439340045\n",
      "state: [-0.9888356   0.14901075 -2.7956378 ]\n",
      "reward: -10.4387030903325\n",
      "state: [-0.95570165  0.29433706 -2.9838798 ]\n",
      "reward: -9.737771944145994\n",
      "state: [-0.8996111  0.4366919 -3.063127 ]\n",
      "reward: -8.97604128718812\n",
      "state: [-0.82324165  0.56769115 -3.035608  ]\n",
      "reward: -8.176623785638983\n",
      "state: [-0.7322404   0.68104625 -2.9098396 ]\n",
      "reward: -7.366398234186893\n",
      "state: [-0.6339523  0.7733722 -2.699055 ]\n",
      "reward: -6.574304716473083\n",
      "state: [-0.53600836  0.84421265 -2.419026  ]\n",
      "reward: -5.828567539512573\n",
      "state: [-0.4452097   0.89542633 -2.0858665 ]\n",
      "reward: -5.153792724681881\n",
      "state: [-0.36691785  0.9302533  -1.7142967 ]\n",
      "reward: -4.568940084419165\n",
      "state: [-0.30492842  0.9523753  -1.3166066 ]\n",
      "reward: -4.086704355820291\n",
      "state: [-0.2616651   0.96515876 -0.9023252 ]\n",
      "reward: -3.714225821007276\n",
      "state: [-0.23850311  0.97114176 -0.4784562 ]\n",
      "reward: -3.454638401972813\n",
      "state: [-0.23606966  0.97173613 -0.05009989]\n",
      "reward: -3.3088609452179756\n",
      "state: [-0.25442618  0.9670922   0.37870222]\n",
      "reward: -3.2771499888672997\n",
      "state: [-0.2930883   0.9560854   0.80402136]\n",
      "reward: -3.3601106686526627\n",
      "state: [-0.350879   0.9364208  1.2210854]\n",
      "reward: -3.559009468353137\n",
      "state: [-0.42564964  0.90488803  1.623401  ]\n",
      "reward: -3.875326499258087\n",
      "state: [-0.5139499  0.8578202  2.002067 ]\n",
      "reward: -4.30955692388733\n",
      "state: [-0.6107874   0.79179466  2.3454323 ]\n",
      "reward: -4.859372540632258\n",
      "state: [-0.70966214  0.70454216  2.6392782 ]\n",
      "reward: -5.517425222526078\n",
      "state: [-0.8030541   0.59590614  2.8676848 ]\n",
      "reward: -6.269305250503282\n",
      "state: [-0.8834304   0.46856242  3.0146143 ]\n",
      "reward: -7.092367483596439\n",
      "state: [-0.9446202   0.32816562  3.0660362 ]\n",
      "reward: -7.956133618205481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-664.5395914954541"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute 10 episodes with Heuristic agent\n",
    "heur_ag = HeuristicPendulumAgent(norm_env)\n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(heur_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3a93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
