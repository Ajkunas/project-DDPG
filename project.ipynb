{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e31bf8",
   "metadata": {},
   "source": [
    "## Control in a continuous action space with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dabef",
   "metadata": {},
   "source": [
    "### Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94bfa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "from matplotlib import pyplot\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d2e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "norm_env = NormalizedEnv(env) # accept actions between -1 and 1\n",
    "\n",
    "rand_ag = RandomAgent(norm_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one episode with a defined number of steps\n",
    "\n",
    "def episode(agent): \n",
    "    state, info = norm_env.reset()\n",
    "    tot_reward = 0\n",
    "    truncated = False\n",
    "\n",
    "    while not truncated:\n",
    "        action = agent.compute_action(state)\n",
    "        next_state, reward, terminated, truncated, info = norm_env.step(action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        if truncated:\n",
    "            state, info = norm_env.reset()\n",
    "            \n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73172ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-957.1909959975984, -815.4421460613864, -1564.6719613777366, -1758.497453884742, -1298.3150471935508, -1626.8967524486884, -936.8622448210249, -890.5947654293064, -990.9154222228802, -1169.0553501677819]\n",
      "average cumulative reward : -1200.8442139604697\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes \n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(rand_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a heuristic policy for the pendulum\n",
    "    \n",
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        _, y, v = state\n",
    "        torque = env.m*env.g*env.l*y # fixed torque\n",
    "        action = np.empty((1,))\n",
    "        \n",
    "        if (y < 0):\n",
    "            np.append(action, np.sign(v)*torque) # same direction to angular velocity\n",
    "        else:\n",
    "            np.append(action, (-1)*np.sign(v)*torque) # opposite direction to angular velocity\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2726e186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1650.090527028244, -1372.780317373032, -1652.5517983907177, -1672.2371923926673, -1659.5844213967787, -1650.1917024590014, -1347.4898405672827, -1613.6774356330363, -1064.8552873614594, -1652.4848704942667]\n",
      "average cumulative reward : -1533.5943393096488\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes with Heuristic agent\n",
    "heur_ag = HeuristicPendulumAgent(norm_env)\n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(heur_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09cb90",
   "metadata": {},
   "source": [
    "## QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e3dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qnetwork import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9fcc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3975528/3438573092.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  action = torch.Tensor([action])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "gamma = 0.01\n",
    "model = QNetwork(heur_ag, norm_env)\n",
    "\n",
    "state, info = norm_env.reset()\n",
    "\n",
    "action = heur_ag.compute_action(state)\n",
    "\n",
    "action = torch.Tensor([action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c87dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.2026, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_t = torch.Tensor([state, state, state])\n",
    "action_t = torch.Tensor([action, action, action]).view(-1, 1)\n",
    "\n",
    "transition = torch.cat([state_t, action_t], dim=1)\n",
    "model.update(transition, gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7136b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8e682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7735e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37812f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f8c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f185204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
