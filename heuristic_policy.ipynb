{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e31bf8",
   "metadata": {},
   "source": [
    "## Control in a continuous action space with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dabef",
   "metadata": {},
   "source": [
    "### Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94bfa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d2e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "norm_env = NormalizedEnv(env) # accept actions between -1 and 1\n",
    "\n",
    "rand_ag = RandomAgent(norm_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one episode with a defined number of steps\n",
    "\n",
    "def episode(agent): \n",
    "    state, info = norm_env.reset()\n",
    "    tot_reward = 0\n",
    "    truncated = False\n",
    "\n",
    "    while not truncated:\n",
    "        action = agent.compute_action(state)\n",
    "        next_state, reward, terminated, truncated, info = norm_env.step(action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        if truncated:\n",
    "            state, info = norm_env.reset()\n",
    "            \n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73172ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1170.3079063476066, -1572.9461396799957, -1082.5399920424336, -886.2308836962088, -956.6122258191956, -1530.8991488636425, -862.9167104440036, -1430.8567780097287, -1532.548876263106, -1749.4058851088723]\n",
      "average cumulative reward : -1277.5264546274793\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes \n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(rand_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a heuristic policy for the pendulum\n",
    "\n",
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.env = env\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        _, y, v = state\n",
    "        torque = self.env.m*self.env.g*self.env.l*y # fixed torque\n",
    "        action = np.empty((1,))\n",
    "        \n",
    "        if (y < 0):\n",
    "            np.append(action, np.sign(v)*torque) # same direction to angular velocity\n",
    "        else:\n",
    "            np.append(action, (-1)*np.sign(v)*torque) # opposite direction to angular velocity\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2726e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of reward : [-1579.9818142632985, -1682.6525129540605, -1219.4709965626398, -842.8545238819726, -1630.1069162218646, -1381.0462443169642, -1662.4869628120468, -1655.7169532137818, -1122.723691151543, -1627.6598481532694]\n",
      "average cumulative reward : -1440.470046353144\n"
     ]
    }
   ],
   "source": [
    "# Execute 10 episodes with Heuristic agent\n",
    "heur_ag = HeuristicPendulumAgent(norm_env)\n",
    "\n",
    "arr_reward = []\n",
    "num_ep = 10\n",
    "for x in range(num_ep): \n",
    "    reward = episode(heur_ag)\n",
    "    arr_reward.append(reward)\n",
    "\n",
    "print(\"array of reward :\", arr_reward)\n",
    "\n",
    "av_reward = sum(arr_reward)/num_ep\n",
    "print(\"average cumulative reward :\", av_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8cd89",
   "metadata": {},
   "source": [
    "#### How does it compare with the reward of the random agent ?\n",
    "\n",
    "We notice that the rewards at each episode are more stable than the ones while use the random agent. With the random agent, the reward fluctuates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe4148",
   "metadata": {},
   "source": [
    "#### What impact does the amplitude of the fixed torque have on the reward ?\n",
    "\n",
    "The amplitude of the torque gives the necessary force to the pendulum to be at least in the correct domain (upper domain). Therefore, the rewards are more stable at each episode. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
